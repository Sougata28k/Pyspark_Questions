{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most profitable company from the financial sector. Output the result along with the continent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assets:(col)\n",
    "double precision\n",
    "company:(col)\n",
    "text\n",
    "continent:(col)\n",
    "text\n",
    "country:(col)\n",
    "text\n",
    "industry:(col)\n",
    "text\n",
    "marketvalue:(col)\n",
    "double precision\n",
    "profits:(col)\n",
    "double precision\n",
    "rank:(col)\n",
    "bigint\n",
    "sales:(col)\n",
    "double precision\n",
    "sector:(col)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "company\tsector\tindustry\tcontinent\tcountry\tmarketvalue\tsales\tprofits\tassets\trank\n",
    "ICBC\tFinancials\tMajor Banks\tAsia\tChina\t215.6\t148.7\t42.7\t3124.9\t1\n",
    "Bank of China\tFinancials\tMajor Banks\tAsia\tChina\t124.2\t105.1\t25.5\t2291.8\t9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Start writing code\n",
    "df = forbes_global_2010_2014.filter(col(\"sector\")== 'Financials')\\\n",
    ".orderBy('profits', ascending= False)\\\n",
    ".limit(1)\\\n",
    ".select(\"company\", \"continent\")\n",
    "\n",
    "\n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 2 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of employees working in the Admin department that joined in April or later.\n",
    "\n",
    "department:\n",
    "text\n",
    "first_name:\n",
    "text\n",
    "joining_date:\n",
    "date\n",
    "last_name:\n",
    "text\n",
    "salary:\n",
    "bigint\n",
    "worker_id:\n",
    "bigint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "worker_id\tfirst_name\tlast_name\tsalary\tjoining_date\tdepartment\n",
    "1\tMonika\tArora\t100000\t2014-02-20\tHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Start writing code\n",
    "df = worker.filter( (month(col(\"joining_date\")) >=4) & (col(\"department\")=='Admin'))\\\n",
    "    .agg(count('*'))\n",
    "\n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question3 (id:10308)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the difference between the highest salaries in the marketing and engineering departments. Output just the absolute difference in salaries.\n",
    "\n",
    "db_employee(table)\n",
    "Preview\n",
    "department_id:\n",
    "bigint\n",
    "first_name:\n",
    "text\n",
    "id:\n",
    "bigint\n",
    "last_name:\n",
    "text\n",
    "salary:\n",
    "bigint\n",
    "\n",
    "\n",
    "db_dept(Table)\n",
    "Preview\n",
    "department:\n",
    "text\n",
    "id:\n",
    "bigint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = db_employee.join(db_dept, db_employee.department_id == db_dept.id, 'inner')\\\n",
    "  .agg(\n",
    "        abs(\n",
    "            max(\n",
    "                when(db_dept['department'] == 'engineering', col('salary'))\n",
    "            ) - \n",
    "            max(\n",
    "                when(db_dept['department'] == 'marketing', col('salary'))\n",
    "            )\n",
    "        ).alias('salary_diff')\n",
    "    )\n",
    "\n",
    "\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most profitable company from the financial sector. Output the result along with the continent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "company:\n",
    "string\n",
    "sector:\n",
    "string\n",
    "industry:\n",
    "string\n",
    "continent:\n",
    "string\n",
    "country:\n",
    "string\n",
    "marketvalue:\n",
    "double\n",
    "sales:\n",
    "double\n",
    "profits:\n",
    "double\n",
    "assets:\n",
    "double\n",
    "rank:\n",
    "bigint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "# Import your libraries\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Start writing code\n",
    "df = forbes_global_2010_2014.filter(col(\"sector\")== 'Financials')\\\n",
    ".orderBy('profits', ascending= False)\\\n",
    ".limit(1)\\\n",
    ".select(\"company\", \"continent\")\n",
    "\n",
    "\n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a DataFrame with a column containing comma-separated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SplitExplodeExample\").getOrCreate()\n",
    "\n",
    "data = [(\"John,Jane,Doe\",), (\"Alice,Bob\",)]\n",
    "df = spark.createDataFrame(data, [\"names\"])\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+-------------+\n",
    "|       names |\n",
    "+-------------+\n",
    "|John,Jane,Doe|\n",
    "|    Alice,Bob|\n",
    "+-------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we apply split to convert the string into an array and then explode to create multiple rows:\n",
    "\n",
    "df_split = df.withColumn(\"name_array\", split(df[\"names\"], \",\"))  # Splitting string into an array\n",
    "df_exploded = df_split.select(explode(df_split[\"name_array\"]).alias(\"name\"))  # Exploding into multiple rows\n",
    "\n",
    "df_exploded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output : +-----+\n",
    "| name|\n",
    "+-----+\n",
    "| John|\n",
    "| Jane|\n",
    "|  Doe|\n",
    "|Alice|\n",
    "|  Bob|\n",
    "+-----+\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
